{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hi'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"hi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(\"exporting\", chunk_name)? (1499897779.py, line 12)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [2]\u001b[1;36m\u001b[0m\n\u001b[1;33m    print \"exporting\", chunk_name\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(\"exporting\", chunk_name)?\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.utils import make_chunks\n",
    "\n",
    "myaudio = AudioSegment.from_file(\"myAudio.wav\" , \"wav\") \n",
    "chunk_length_ms = 5000 # pydub calculates in millisec\n",
    "chunks = make_chunks(myaudio, chunk_length_ms) #Make chunks of one sec\n",
    "\n",
    "#Export all of the individual chunks as wav files\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    chunk_name = \"chunk{0}.wav\".format(i)\n",
    "    print \"exporting\", chunk_name\n",
    "    chunk.export(chunk_name, format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from pydub import AudioSegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y, sr = librosa.load(\"call.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "myaudio = AudioSegment.from_file(\"call.wav\" , \"wav\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"D:\\\\deepspeech\\\\nisqa\\\\NISQA\\\\recordings\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_splitter(audio_dir,split_length=5):\n",
    "    import os\n",
    "    from pydub import AudioSegment\n",
    "    from pydub.utils import make_chunks\n",
    "    chunk_length_ms = 5000 # pydub calculates in millisec\n",
    "    os.listdir(audio_dir)\n",
    "    for filename in os.listdir(audio_dir):\n",
    "        big_chunk = audio_dir+os.sep+ filename\n",
    "        file = AudioSegment.from_file(big_chunk, \"wav\")\n",
    "        if file.duration_seconds > 5:\n",
    "            chunks = make_chunks(file, chunk_length_ms) #Make chunks of five sec\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # print(big_chunk[:-4])\n",
    "                chunk_name = \"{1}_{0}.wav\".format(i, big_chunk[:-4])\n",
    "                # print(audio_dir)\n",
    "                # print(chunk_name)\n",
    "                print(\"exporting\", chunk_name)\n",
    "                chunk.export(os.path.join(audio_dir, chunk_name), format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exporting D:\\deepspeech\\nisqa\\NISQA\\recordings\\call1_0.wav\n",
      "exporting D:\\deepspeech\\nisqa\\NISQA\\recordings\\call1_1.wav\n",
      "exporting D:\\deepspeech\\nisqa\\NISQA\\recordings\\call1_2.wav\n",
      "exporting D:\\deepspeech\\nisqa\\NISQA\\recordings\\call1_3.wav\n"
     ]
    }
   ],
   "source": [
    "chunk_splitter(input_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nisqa = pd.read_csv(\"results/NISQA_results.csv\")\n",
    "df_splitter = pd.read_csv(\"df.csv\")\n",
    "df_splitter = df_splitter.astype({'filename': 'string','transcript':'string', 'accent':'string' })\n",
    "df_nisqa[\"filename\"] = df_nisqa[\"deg\"]\n",
    "df_nisqa = df_nisqa.drop([\"deg\"],axis=1)\n",
    "\n",
    "split_params = ['duration','transcript','confidence','age','gender','accent']\n",
    "# csv_nisqa_params = ['mos_pred', 'nos_pred', 'dis_pred', 'col_pred','loud_pred', 'model']\n",
    "for fields in split_params:\n",
    "    df_nisqa[fields] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nisqa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mos_pred</th>\n",
       "      <th>noi_pred</th>\n",
       "      <th>dis_pred</th>\n",
       "      <th>col_pred</th>\n",
       "      <th>loud_pred</th>\n",
       "      <th>model</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.581293</td>\n",
       "      <td>1.521738</td>\n",
       "      <td>4.343019</td>\n",
       "      <td>3.584870</td>\n",
       "      <td>3.764795</td>\n",
       "      <td>NISQAv2</td>\n",
       "      <td>OutboundSampleRecording_0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.475892</td>\n",
       "      <td>2.258558</td>\n",
       "      <td>3.674884</td>\n",
       "      <td>3.071306</td>\n",
       "      <td>3.260052</td>\n",
       "      <td>NISQAv2</td>\n",
       "      <td>OutboundSampleRecording_1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.635692</td>\n",
       "      <td>1.920747</td>\n",
       "      <td>3.986793</td>\n",
       "      <td>3.092535</td>\n",
       "      <td>3.441731</td>\n",
       "      <td>NISQAv2</td>\n",
       "      <td>OutboundSampleRecording_3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mos_pred  noi_pred  dis_pred  col_pred  loud_pred    model  \\\n",
       "1  2.581293  1.521738  4.343019  3.584870   3.764795  NISQAv2   \n",
       "2  2.475892  2.258558  3.674884  3.071306   3.260052  NISQAv2   \n",
       "3  2.635692  1.920747  3.986793  3.092535   3.441731  NISQAv2   \n",
       "\n",
       "                    filename  \n",
       "1  OutboundSampleRecording_0  \n",
       "2  OutboundSampleRecording_1  \n",
       "3  OutboundSampleRecording_3  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_nisqa[df_nisqa[\"filename\"].str.contains(\"OutboundSampleRecording\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.append(df_nisqa[df_nisqa[\"filename\"].str.contains(\"OutboundSampleRecording\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>traasd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [traasd]\n",
       "Index: []"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_df = pd.DataFrame()\n",
    "params = df_splitter.iloc[1]\n",
    "call_chunks = df_nisqa[df_nisqa[\"deg\"].str.contains(file_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "for i in df_nisqa[df_nisqa['filename'].str.contains(\"call\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split_row in df_splitter.index:\n",
    "    file_length = df_splitter[\"duration\"][i]\n",
    "    transcript = df_splitter[\"transcript\"][i]\n",
    "    file_name = df_splitter[\"filename\"][i]\n",
    "    if file_length>5:\n",
    "        # nisqa_params = df_nisqa[df_nisqa['filename'].str.contains(file_name)]\n",
    "        chunk_split_params = df_splitter.iloc[i]\n",
    "        df_nisqa[df_nisqa['filename'].str.contains(file_name)]\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Himanshu\\AppData\\Local\\Temp\\ipykernel_13372\\1070831002.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  call_chunks[\"transcriptt\"]=[\"\"]\n",
      "C:\\Users\\Himanshu\\AppData\\Local\\Temp\\ipykernel_13372\\1070831002.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  call_chunks.loc[j,\"transcriptt\"] = d[j]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible indexer with Series",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\deepspeech\\nisqa\\NISQA\\splitter.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deepspeech/nisqa/NISQA/splitter.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(final)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/deepspeech/nisqa/NISQA/splitter.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     d[key] \u001b[39m=\u001b[39m final[key]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/deepspeech/nisqa/NISQA/splitter.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m call_chunks\u001b[39m.\u001b[39;49mloc[j,\u001b[39m\"\u001b[39;49m\u001b[39mtranscriptt\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39m=\u001b[39m d[j]\n",
      "File \u001b[1;32mc:\\Users\\Himanshu\\anaconda3\\envs\\nisqa\\lib\\site-packages\\pandas\\core\\indexing.py:723\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    722\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 723\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Himanshu\\anaconda3\\envs\\nisqa\\lib\\site-packages\\pandas\\core\\indexing.py:1730\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1727\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n\u001b[0;32m   1728\u001b[0m \u001b[39mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1729\u001b[0m     \u001b[39m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1730\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1731\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1732\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32mc:\\Users\\Himanshu\\anaconda3\\envs\\nisqa\\lib\\site-packages\\pandas\\core\\indexing.py:1751\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1748\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39misinstance\u001b[39m(value, ABCSeries) \u001b[39mand\u001b[39;00m name \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(value, \u001b[39mdict\u001b[39m):\n\u001b[0;32m   1749\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mimport\u001b[39;00m Series\n\u001b[1;32m-> 1751\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_align_series(indexer, Series(value))\n\u001b[0;32m   1753\u001b[0m \u001b[39m# Ensure we have something we can iterate over\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m info_axis \u001b[39m=\u001b[39m indexer[\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Himanshu\\anaconda3\\envs\\nisqa\\lib\\site-packages\\pandas\\core\\indexing.py:2149\u001b[0m, in \u001b[0;36m_iLocIndexer._align_series\u001b[1;34m(self, indexer, ser, multiindex_indexer)\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39m_values\u001b[39m.\u001b[39mcopy()\n\u001b[0;32m   2147\u001b[0m     \u001b[39mreturn\u001b[39;00m ser\u001b[39m.\u001b[39mreindex(ax)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 2149\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mIncompatible indexer with Series\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible indexer with Series"
     ]
    }
   ],
   "source": [
    "for i in df_splitter.index:\n",
    "    \n",
    "    transcript = df_splitter[\"transcript\"][i]\n",
    "    file_name = df_splitter[\"filename\"][i]\n",
    "    if file_length > 5:\n",
    "        file_parameters = df_splitter.iloc[i]\n",
    "        call_chunks = df_nisqa[df_nisqa[\"filename\"].str.contains(file_name)]\n",
    "        call_chunks[\"transcriptt\"]=[\"\"]\n",
    "        #add transcipts using number of words/duration in sec in candidate chunks\n",
    "        if transcript == \"\":\n",
    "            for j in range(call_chunks.shape[0]):\n",
    "                call_chunks.iloc[i][\"transcriptt\"] = \"Null\"\n",
    "        else:\n",
    "            for j in range(call_chunks.shape[0]):\n",
    "                n = int(len(transcript.split())/call_chunks.shape[0])\n",
    "                final = [call_chunks[\"transcriptt\"][_ * n:(_ + 1) * n] for _ in range((call_chunks.shape[0] + n - 1) // n )]\n",
    "                d = {}\n",
    "                for key in range(len(final)):\n",
    "                    d[key] = final[key]\n",
    "                call_chunks.loc[j,\"transcriptt\"] = d[j]\n",
    "\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4 entries, 0 to 3\n",
      "Data columns (total 12 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Unnamed: 0     4 non-null      int64  \n",
      " 1   duration       4 non-null      int64  \n",
      " 2   filename       4 non-null      object \n",
      " 3   transcript     4 non-null      object \n",
      " 4   confidence     4 non-null      float64\n",
      " 5   loudness       4 non-null      int64  \n",
      " 6   noisiness      4 non-null      int64  \n",
      " 7   coloration     4 non-null      int64  \n",
      " 8   discontinuity  4 non-null      int64  \n",
      " 9   age            4 non-null      int64  \n",
      " 10  gender         4 non-null      int64  \n",
      " 11  accent         4 non-null      int64  \n",
      "dtypes: float64(1), int64(9), object(2)\n",
      "memory usage: 512.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df_splitter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values(by=[\"name\"])\n",
    "df =df.reset_index()\n",
    "df.drop([\"index\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splitter = pd.read_csv(\"df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splitter = pd.read_csv(\"df.csv\")\n",
    "df_nisqa = pd.read_csv(\"results/NISQA_results.csv\")\n",
    "df_nisqa[\"filename\"] = df_nisqa[\"deg\"]\n",
    "df_nisqa.drop([\"deg\"],axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hh\n",
      "hh\n"
     ]
    }
   ],
   "source": [
    "for i in df_splitter.index:\n",
    "        file_length = df_splitter[\"duration\"][i]\n",
    "        file_name = df_splitter[\"filename\"][i]\n",
    "        if file_length > 5:\n",
    "            # get its parameters in a list [\"age\", \"gender\", etc etc] or get the entire row in a list\n",
    "            # delete that entry using pd.drop\n",
    "            same_parameters = df_splitter.iloc[i] #can use .values attribute too\n",
    "            df_splitter = df_splitter[df_splitter.index != i]\n",
    "            \n",
    "            # append it to df_splitter and sort it and reset index using \n",
    "            # df = df.sort_values(by=[\"name\"])\n",
    "            # df =df.reset_index()\n",
    "            # df.drop([\"index\"], axis=1)\n",
    "            # nisqa_parameters = Get entries having filename same as file length's name\n",
    "            # append it to df_splitter\n",
    "        else:\n",
    "            # we know the duration is < 5 and we can simply copy and paste the nisqa parameter in file_length's name row\n",
    "            print(\"hh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "call_chunks = df_nisqa[df_nisqa[\"filename\"].str.contains(file_name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splitter = df_splitter.append(call_chunks, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>duration</th>\n",
       "      <th>filename</th>\n",
       "      <th>transcript</th>\n",
       "      <th>confidence</th>\n",
       "      <th>loudness</th>\n",
       "      <th>noisiness</th>\n",
       "      <th>coloration</th>\n",
       "      <th>discontinuity</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "      <th>mos_pred</th>\n",
       "      <th>noi_pred</th>\n",
       "      <th>dis_pred</th>\n",
       "      <th>col_pred</th>\n",
       "      <th>loud_pred</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>OutboundSampleRecording_0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.789234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>OutboundSampleRecording_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-6.178956</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>OutboundSampleRecording_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.312910</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>OutboundSampleRecording_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-15.193325</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OutboundSampleRecording_3_1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.635692</td>\n",
       "      <td>1.920747</td>\n",
       "      <td>3.986793</td>\n",
       "      <td>3.092535</td>\n",
       "      <td>3.441731</td>\n",
       "      <td>NISQAv2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OutboundSampleRecording_3_2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.189593</td>\n",
       "      <td>2.064310</td>\n",
       "      <td>3.807749</td>\n",
       "      <td>3.011560</td>\n",
       "      <td>3.009693</td>\n",
       "      <td>NISQAv2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OutboundSampleRecording_3_3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.058475</td>\n",
       "      <td>2.106824</td>\n",
       "      <td>3.524250</td>\n",
       "      <td>2.770824</td>\n",
       "      <td>3.104320</td>\n",
       "      <td>NISQAv2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  duration                     filename  transcript  confidence  \\\n",
       "0         0.0      12.0    OutboundSampleRecording_0         NaN   -3.789234   \n",
       "1         1.0       4.0    OutboundSampleRecording_1         NaN   -6.178956   \n",
       "2         2.0       6.0    OutboundSampleRecording_2         NaN   -3.312910   \n",
       "3         3.0       5.0    OutboundSampleRecording_3         NaN  -15.193325   \n",
       "4         NaN       NaN  OutboundSampleRecording_3_1         NaN         NaN   \n",
       "5         NaN       NaN  OutboundSampleRecording_3_2         NaN         NaN   \n",
       "6         NaN       NaN  OutboundSampleRecording_3_3         NaN         NaN   \n",
       "\n",
       "   loudness  noisiness  coloration  discontinuity  age  gender  accent  \\\n",
       "0       0.0        0.0         0.0            0.0  0.0     0.0     0.0   \n",
       "1       0.0        0.0         0.0            0.0  0.0     0.0     0.0   \n",
       "2       0.0        0.0         0.0            0.0  0.0     0.0     0.0   \n",
       "3       0.0        0.0         0.0            0.0  0.0     0.0     0.0   \n",
       "4       NaN        NaN         NaN            NaN  NaN     NaN     NaN   \n",
       "5       NaN        NaN         NaN            NaN  NaN     NaN     NaN   \n",
       "6       NaN        NaN         NaN            NaN  NaN     NaN     NaN   \n",
       "\n",
       "   mos_pred  noi_pred  dis_pred  col_pred  loud_pred    model  \n",
       "0       NaN       NaN       NaN       NaN        NaN      NaN  \n",
       "1       NaN       NaN       NaN       NaN        NaN      NaN  \n",
       "2       NaN       NaN       NaN       NaN        NaN      NaN  \n",
       "3       NaN       NaN       NaN       NaN        NaN      NaN  \n",
       "4  2.635692  1.920747  3.986793  3.092535   3.441731  NISQAv2  \n",
       "5  2.189593  2.064310  3.807749  3.011560   3.009693  NISQAv2  \n",
       "6  2.058475  2.106824  3.524250  2.770824   3.104320  NISQAv2  "
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "shutil.copy(os.path.join(audio_dir, filename), os.path.join(output_dir, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = \"this is what has been going on forever and my name has been going on and you have been avoiding me at that time\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is what has been going on forever', 'and my name has been going on and', 'you have been avoiding me at that time']\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "length_of_audio = 12\n",
    "num_of_subaudios = ceil(length_of_audio/ 5)\n",
    "chunk_size = ceil(len(my_list)/num_of_subaudios)\n",
    "list_chunked = [my_list[i:i + chunk_size] for i in range(0, len(my_list), chunk_size)]\n",
    "sentence = []\n",
    "for word in list_chunked:\n",
    "    sentence.append(\" \".join(word))\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this is what has been going', 'on forever and my name has', 'been going on and you have', 'been avoiding me at that time']\n",
      "0 this is what has been going\n",
      "1 on forever and my name has\n",
      "2 been going on and you have\n",
      "3 been avoiding me at that time\n"
     ]
    }
   ],
   "source": [
    "calllist = [\"call_0_1\", \"call_0_2\", \"call_0_3\", \"call_0_4\", \"call some\"]\n",
    "my_list = \"this is what has been going on forever and my name has been going on and you have been avoiding me at that time\".split()\n",
    "from math import ceil\n",
    "counter = 0\n",
    "length_of_audio = 17\n",
    "num_of_subaudios = ceil(length_of_audio/ 5)\n",
    "chunk_size = ceil(len(my_list)/num_of_subaudios)\n",
    "list_chunked = [my_list[i:i + chunk_size] for i in range(0, len(my_list), chunk_size)]\n",
    "sentence = []\n",
    "for word in list_chunked:\n",
    "    sentence.append(\" \".join(word))\n",
    "print(sentence)\n",
    "for i in calllist:\n",
    "\n",
    "    if \"call_0\" in i:\n",
    "        # same_flag = True\n",
    "        print(counter, sentence[counter])\n",
    "        counter+=1\n",
    "        # same_flag = False\n",
    "    else:\n",
    "        counter = 0\n",
    "        # same_flag = False\n",
    "        # del counter\n",
    "    # print(counter, sentence[counter])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 ('nisqa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42feb79fa54bc67696343d87c2233fc35a6874b7a7f7d28363247e7c5e2a5a81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
