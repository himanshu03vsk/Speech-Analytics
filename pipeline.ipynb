{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diarization: <br>\n",
    "Look for audio in data directory<br>\n",
    "Take that audiofile and diarize them(Can be optimized)<br>\n",
    "Inititalize the STT<br>\n",
    "split the full call according to the timings<br>\n",
    "Use the splitted audio to perform nisqa assessment<br>\n",
    "(is there any way to get nisqa scores directly without them being saved in csv?)<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "#This is for making the notebook get updated files after editing somefiles in the working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\himan\\anaconda3\\envs\\nisqa\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import utils\n",
    "from Diarisation_STT import utils\n",
    "import os\n",
    "import glob\n",
    "import wget\n",
    "from pyannote.audio import Audio \n",
    "from IPython.display import Audio as IPythonAudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = os.getcwd()\n",
    "data_dir = os.path.join(ROOT,'Diarisation_STT\\data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "audio_file_list = glob.glob(f\"{data_dir}/*.wav\") #This is will return all the .wav files in the data directory\n",
    "# print(audio_file_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If facing import error:<br>\n",
    "so files need to be in stt/ instead of stt/lib/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n",
      "The torchaudio backend is switched to 'soundfile'. Note that 'sox_io' is not supported on Windows.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized the stt model successfully\n"
     ]
    }
   ],
   "source": [
    "# instantiate pretrained speaker diarization pipeline\n",
    "pipeline = utils.init_diarize_pipline()\n",
    "model, ds_model = utils.init_model_stt(\"Diarisation_STT\\model.tflite\", \"Diarisation_STT\\large_vocabulary.scorer\")\n",
    "# apply pretrained pipeline\n",
    "# ds model is deepspeech model\n",
    "# model uses coqui\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:\\\\Users\\\\himan\\\\Documents\\\\Speech\\\\Diarisation_STT\\\\Timings\\\\c:\\\\Users\\\\himan\\\\Documents\\\\Speech\\\\Diarisation_STT\\\\data\\\\OutboundSampleRecording.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\himan\\Documents\\Speech\\pipeline.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/himan/Documents/Speech/pipeline.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mcreate_df([\u001b[39m\"\u001b[39m\u001b[39mfilename\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtranscript\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mconfidence\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mloudness\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnoisiness\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcoloration\u001b[39m\u001b[39m\"\u001b[39m , \u001b[39m\"\u001b[39m\u001b[39mdiscontinuity\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mage\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39maccent\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/himan/Documents/Speech/pipeline.ipynb#W5sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m utils\u001b[39m.\u001b[39;49mdiarize(audio_file_list, pipeline, output_dir\u001b[39m=\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mhiman\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSpeech\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDiarisation_STT\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mTimings\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\himan\\Documents\\Speech\\Diarisation_STT\\utils.py:103\u001b[0m, in \u001b[0;36mdiarize\u001b[1;34m(audio_file_list, pipeline, output_dir)\u001b[0m\n\u001b[0;32m     98\u001b[0m diarization \u001b[39m=\u001b[39m pipeline(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m turn, _, speaker \u001b[39min\u001b[39;00m diarization\u001b[39m.\u001b[39mitertracks(yield_label\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m    100\u001b[0m   \u001b[39m#if speaker ==\"SPEAKER_01\":\u001b[39;00m\n\u001b[0;32m    101\u001b[0m   \u001b[39m#  continue\u001b[39;00m\n\u001b[0;32m    102\u001b[0m   \u001b[39m# print(f\"{i}: start={turn.start:.1f}s stop={turn.end:.1f}s speaker_{speaker}\")\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m   \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mtiming_path\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mos\u001b[39m.\u001b[39;49msep\u001b[39m}\u001b[39;49;00m\u001b[39m{\u001b[39;49;00mfile\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m][:\u001b[39m-\u001b[39;49m\u001b[39m4\u001b[39;49m]\u001b[39m}\u001b[39;49;00m\u001b[39m.txt\u001b[39;49m\u001b[39m\"\u001b[39;49m ,\u001b[39m\"\u001b[39;49m\u001b[39ma\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    104\u001b[0m     f\u001b[39m.\u001b[39mwritelines((\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mturn\u001b[39m.\u001b[39mstart\u001b[39m:\u001b[39;00m\u001b[39m.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mturn\u001b[39m.\u001b[39mend\u001b[39m:\u001b[39;00m\u001b[39m.1f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mspeaker\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m))\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'C:\\\\Users\\\\himan\\\\Documents\\\\Speech\\\\Diarisation_STT\\\\Timings\\\\c:\\\\Users\\\\himan\\\\Documents\\\\Speech\\\\Diarisation_STT\\\\data\\\\OutboundSampleRecording.txt'"
     ]
    }
   ],
   "source": [
    "df = utils.create_df([\"filename\", \"transcript\", \"confidence\", \"loudness\", \"noisiness\", \"coloration\" , \"discontinuity\", \"age\", \"gender\", \"accent\"])\n",
    "utils.diarize(audio_file_list, pipeline, output_dir=r\"C:\\Users\\himan\\Documents\\Speech\\Diarisation_STT\\Timings\") #This diarizes the audio calls in data_dir and they will be stored as timings folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass model or ds_model to use either of the stt engine to see results\n",
    "df_new = utils.audio_splitter(ROOT, data_dir=data_dir, audio_file_list=audio_file_list, df=df ,model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('nisqa')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dffa8045830cef074a493d371bc3f135f3a7796621687ab04b5ebf9d31373b37"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
